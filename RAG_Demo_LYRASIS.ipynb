{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# üéØ RAG for Libraries: Live Demo + Playground\n\n**Two modes in one notebook:**\n\n## üé¨ PART 1: LIVE DEMO (Top)\n**As mentioned in the presentation** - this is what we're demoing: the four steps of RAG in action\nStreamlined for presentations - 5 cells, ~3-4 minutes\n\n## üî¨ PART 2: TINKERER'S PLAYGROUND (Bottom)\nDeep dive with explanations and extra visualizations that mirror Act II of the presentation\n\n**No API keys required for demo** | **Free to run** | **Run cells with ‚ñ∂Ô∏è**\n\n---\n\n*For LYRASIS presentation | [GitHub](https://github.com/radio-shaq/Lyrasis-slides-11-2025)*"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üé¨ PART 1: LIVE DEMO\n",
    "\n",
    "**For presentations:** Run these 5 cells in sequence.\n",
    "\n",
    "**What you'll see:**\n",
    "- üìä Beautiful embedding visualization\n",
    "- üÜö Side-by-side RAG vs no-RAG comparison\n",
    "- ‚ú® The \"wow\" moment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Demo Step 0: Preliminary Setup (30 sec)\n**This is an interactive way to use Python** - runs in Google's cloud, safe to use\n\n**What this does:** Installs libraries and imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers chromadb pandas numpy scikit-learn matplotlib seaborn\n",
    "\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"‚úÖ Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Demo Step 1: INGEST - Upload Your Documents\n**From the presentation: Step 1 of RAG** - Load your library's FAQs, policies, guides\n\n**You can paste your own information here** - CSV format (plain text spreadsheet)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = \"\"\"question,answer,category\n",
    "\"What are the library hours?\",\"The library is open Monday-Friday 8:00 AM to 10:00 PM, Saturday 10:00 AM to 6:00 PM, and Sunday 12:00 PM to 8:00 PM. Hours may vary during holidays.\",hours\n",
    "\"When does the library close?\",\"Regular closing times are 10:00 PM Monday-Friday, 6:00 PM Saturday, and 8:00 PM Sunday. During finals week we extend to midnight.\",hours\n",
    "\"Are you open on weekends?\",\"Yes! We're open Saturdays 10:00 AM - 6:00 PM and Sundays 12:00 PM - 8:00 PM.\",hours\n",
    "\"What are your holiday hours?\",\"The library follows the university calendar. We're typically closed on major holidays and have reduced hours during breaks.\",hours\n",
    "\"Do you have 24-hour study spaces?\",\"We have a designated 24-hour study lounge on the second floor accessible with your student ID.\",hours\n",
    "\"How do I reserve a study room?\",\"Study rooms can be reserved through our online booking system at libcal.yourlibrary.edu. Rooms are available in 2-hour blocks.\",facilities\n",
    "\"Can I book a group study room?\",\"Yes! Group study rooms (4-8 people) can be booked online up to 7 days in advance.\",facilities\n",
    "\"Do you have private study spaces?\",\"We have individual study carrels on the third floor, first-come first-served.\",facilities\n",
    "\"Is there a quiet study area?\",\"Yes, the third floor is silent study. Second floor allows quiet conversation. First floor is collaborative.\",facilities\n",
    "\"Can I eat in the library?\",\"Light snacks and drinks with secure lids permitted on all floors except Archives. No hot food or open containers.\",facilities\n",
    "\"How do I access databases from home?\",\"Access all library databases off-campus by logging in with your university credentials when prompted.\",database\n",
    "\"Why can't I access JSTOR from home?\",\"Make sure you're using the library's link and entering full university credentials. Clear browser cache if needed.\",database\n",
    "\"Do I need a VPN to use library resources?\",\"No VPN needed! Our databases use proxy authentication through the library website.\",database\n",
    "\"How do I find peer-reviewed articles?\",\"Use databases like JSTOR or ProQuest. Most have a filter for peer-reviewed sources.\",database\n",
    "\"What's the difference between a database and Google Scholar?\",\"Library databases provide subscription access with better filtering and guaranteed full-text access.\",database\n",
    "\"How many books can I check out?\",\"Undergrads: 20 books. Graduate students: 50 books. Faculty limits vary.\",policies\n",
    "\"How long can I keep a book?\",\"3-week loan period with one renewal if no one else requested it.\",policies\n",
    "\"Can I renew my books?\",\"Yes! Renew online through your library account or call circulation desk. Once unless on hold.\",policies\n",
    "\"What happens if I return a book late?\",\"$0.25/day per item, max $10. Items 30+ days overdue are considered lost.\",policies\n",
    "\"Can I renew if it's late?\",\"Yes if no one requested it. You still owe fines for overdue days.\",policies\n",
    "\"Do you have textbooks?\",\"Limited textbooks on Course Reserve for in-library or overnight checkout.\",collections\n",
    "\"How do I request a book from another library?\",\"Use Interlibrary Loan! Log in to your ILL account. Most items arrive in 5-10 business days.\",services\n",
    "\"Can I get articles from other universities?\",\"Yes, through ILL. PDF usually within 2-3 business days.\",services\n",
    "\"Is there a scanner I can use?\",\"Yes! Flatbed and large-format scanners on all floors. Scan-to-email on copiers. All free.\",technology\n",
    "\"Do you have laptops I can borrow?\",\"Yes, 4-hour checkout at circulation desk with student ID. Windows and Mac available.\",technology\n",
    "\"\"\"\n",
    "\n",
    "data = pd.read_csv(StringIO(csv_data))\n",
    "print(f\"‚úÖ Loaded {len(data)} FAQs\")\n",
    "print(f\"üìä Categories: {', '.join(data['category'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Demo Step 2: STORE - Create Embeddings & Visualize üìä\n**Remember the GPS coordinates analogy?** Here's how it actually works - creating meaning-based coordinates\n\n**Accessibility:** Color + shape markers for universal design"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create embeddings\nembedder = SentenceTransformer('all-MiniLM-L6-v2')\ntexts = [f\"Q: {row['question']} A: {row['answer']}\" for _, row in data.iterrows()]\nembeddings = embedder.encode(texts, show_progress_bar=True)\n\n# Reduce to 2D for visualization\npca = PCA(n_components=2)\nembeddings_2d = pca.fit_transform(embeddings)\n\n# Accessibility: Use different markers AND colors (universal design)\nmarkers = ['o', 's', '^', 'D', 'v', '*', 'P', 'X']  # circle, square, triangle, diamond, etc.\n\n# Beautiful plot with accessibility\nplt.figure(figsize=(14, 9))\ncategories = data['category'].unique()\ncolors = plt.cm.Set3(range(len(categories)))\n\nfor i, cat in enumerate(categories):\n    mask = data['category'] == cat\n    marker = markers[i % len(markers)]  # Cycle through markers\n    plt.scatter(\n        embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n        c=[colors[i]], label=cat.title(),\n        marker=marker,  # Different shape for each category\n        s=200, alpha=0.7, edgecolors='black', linewidth=2\n    )\n\nplt.title('üß† Similar FAQs Cluster Together', fontsize=18, fontweight='bold', pad=20)\nplt.xlabel('Dimension 1', fontsize=14)\nplt.ylabel('Dimension 2', fontsize=14)\nplt.legend(title='Category', fontsize=12, title_fontsize=13, loc='best')\nplt.grid(alpha=0.3, linestyle='--')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nüí° Each point = 1 FAQ. Similar topics cluster together!\")\nprint(\"‚ôø Accessibility: Different shapes + colors (not color alone)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Demo Step 3: Build the Semantic Card Catalog\n**This is the 'semantic card catalog' concept from the slides** - storing FAQs by meaning, not keywords\n\n**What this does:** Creates vector database with ChromaDB"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create vector DB (or get existing one)\nclient = chromadb.EphemeralClient()\ncollection = client.get_or_create_collection(name=\"library_faqs\")\n\n# Clear existing data if re-running\ntry:\n    collection.delete(ids=[f\"faq_{i}\" for i in range(100)])  # Delete any old data\nexcept:\n    pass  # Collection was empty, that's fine\n\n# Add FAQs to our semantic card catalog\ncollection.add(\n    embeddings=[emb.tolist() for emb in embeddings],\n    documents=data['answer'].tolist(),\n    metadatas=[{\"question\": row['question'], \"category\": row['category']} \n               for _, row in data.iterrows()],\n    ids=[f\"faq_{i}\" for i in range(len(data))]\n)\n\nprint(f\"‚úÖ Semantic card catalog ready with {collection.count()} FAQs\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Demo Step 4: RETRIEVE + GENERATE üÜö\n**Steps 3 & 4 from presentation:** Reference interview (find relevant docs) + AI answers with citations\n\n**See the difference:** Grounded and verifiable vs. generic guessing"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def rag_answer(question):\n    # RETRIEVE: Like doing a reference interview + browsing the right shelf\n    query_emb = embedder.encode([question])[0]\n    results = collection.query(query_embeddings=[query_emb.tolist()], n_results=1)\n    if results['documents'][0]:\n        answer = results['documents'][0][0]\n        source = results['metadatas'][0][0]['question']\n        # GENERATE: Return answer with citation - grounded and verifiable!\n        return f\"{answer}\\n\\nüìö Source: '{source}'\"\n    return \"No info in knowledge base.\"\n\ndef no_rag_answer(question):\n    return \"The library is typically open during regular business hours. Exact times may vary. Check with your library for accurate information.\"\n\n# DEMO QUESTION\nq = \"Can I bring food into the library?\"\n\nprint(\"=\" * 80)\nprint(f\"‚ùì QUESTION: {q}\")\nprint(\"=\" * 80)\n\nprint(\"\\nüî¥ WITHOUT RAG (15-30% hallucination rate):\\n\")\nprint(no_rag_answer(q))\nprint(\"\\n‚ö†Ô∏è  Vague. Generic. Might be wrong. NO SOURCE.\\n\")\n\nprint(\"=\" * 80)\n\nprint(\"\\nüü¢ WITH RAG (2-5% hallucination rate - 6.5x more reliable):\\n\")\nprint(rag_answer(q))\nprint(\"\\n‚úÖ Specific. Accurate. Cited. GROUNDED AND VERIFIABLE.\\n\")\n\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# üî¨ PART 2: TINKERER'S PLAYGROUND\n\n**Explore deeper!**\n\nThis section mirrors **Act II of the presentation** (How RAG Works) with:\n- üß† Clear explanations using library analogies\n- üìä Visualizations to understand your data\n- üéÆ Interactive query testing\n- ü§ñ Real LLM integration (OpenAI & Google Gemini)\n- üìö Resources & next steps\n\n**Why this matters:** Part 1 showed you *what* RAG does. Part 2 shows you *how* it works and *how* to customize it.\n\n**Remember from the presentation:** Your information literacy skills apply here - evaluating sources, understanding queries, citing accurately.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üß† What is RAG? (Clear Explanation)\n\n### The Problem: LLMs Don't Know Your Library\n\nWhen you ask ChatGPT \"What are your library hours?\" it will:\n- ‚ùå Make up generic hours\n- ‚ùå Give outdated information (trained on data from 2021-2023)\n- ‚ùå Provide no sources\n- ‚ùå Hallucinate 15-30% of the time (TruthfulQA, 2022; Watanabe et al., 2025)\n\n**Why?** As mentioned in the presentation: LLMs are trained on the internet, but have no access to YOUR library's current policies.\n\n### The Solution: RAG (Retrieval-Augmented Generation)\n\n**From the presentation:** Think of RAG as giving AI an \"open book test\" instead of asking it to memorize everything.\n\n**WITHOUT RAG (Closed-book exam):**\n```\nQuestion ‚Üí LLM Memory ‚Üí Guessed Answer ‚ùå\n```\n\n**WITH RAG (Open-book exam):**\n```\nQuestion ‚Üí Search Your FAQs ‚Üí Find Top Matches ‚Üí \nGive Them to LLM ‚Üí Answer Based on YOUR Docs ‚úÖ\n```\n\n### The Four Steps of RAG (from Slide 8):\n\n1. **INGEST** ‚Üí Upload your documents (policies, FAQs, catalog data)\n2. **STORE** ‚Üí Create semantic card catalog (vector embeddings)\n3. **RETRIEVE** ‚Üí Find relevant sources (like reference interview + shelf browsing)\n4. **GENERATE** ‚Üí AI answers using ONLY retrieved context + cites sources\n\n### Why RAG is Better - The Statistics:\n\nAs shown in the presentation (Slide 3):\n- ‚úÖ **Grounded in YOUR docs** - Uses your actual policies\n- ‚úÖ **Shows sources** - Cites which FAQ it used\n- ‚úÖ **Says \"I don't know\"** - When it can't find an answer\n- ‚úÖ **2-5% hallucination rate** - Instead of 15-30% (6.5x more reliable!)\n- ‚úÖ **Always current** - Update your FAQs, update the answers\n\n**ACRL framework:** \"RAG enhances generative AI by drawing on external sources... allowing outputs to be more grounded and verifiable.\" (ACRL AI Competencies, 2025)\n\n**Big idea:** RAG = Research assistant with a library card to YOUR collection."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üî¢ Understanding Embeddings (GPS Coordinates for Meaning)\n\n**From the presentation:** Embeddings = GPS coordinates for concepts\n\n### Why We Need Embeddings:\n\nComputers can't understand \"What are your hours?\" vs \"When are you open?\" means the same thing.\n\nBut if we convert them to numbers (embeddings), the computer can see they're similar!\n\nThis enables **semantic search** - searching by meaning, not just keywords. Your cataloging and metadata skills apply here!\n\n### How It Works:\n\n- **\"What are your hours?\"** ‚Üí [0.42, -0.13, 0.87, ...] (384 numbers)\n- **\"When are you open?\"** ‚Üí [0.39, -0.15, 0.85, ...] (similar numbers!)\n- **\"How do I renew a book?\"** ‚Üí [0.91, 0.22, -0.34, ...] (different numbers)\n\n**The magic:** Different words, same meaning = similar embedding vectors (nearby in vector space)\n\n**The analogy from Slide 11:** Like GPS coordinates on Earth - nearby coordinates = nearby locations. Nearby vectors = nearby meanings.\n\n### This Creates Your Semantic Card Catalog:\n\nJust like a traditional card catalog organized books by subject, author, and title - embeddings organize your FAQs by **meaning**.\n\n- Traditional catalog: Books physically near each other on shelf by call number\n- Semantic catalog: FAQs \"near\" each other in vector space by meaning\n\n**Your expertise matters:** Like you evaluate subject headings and controlled vocabularies, you can evaluate whether similar questions are truly semantically related."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# See similarity in action - this is the math behind semantic search\nfrom numpy.linalg import norm\n\ndef similarity(a, b):\n    \"\"\"Calculate cosine similarity between two vectors (0-1, higher = more similar)\"\"\"\n    return np.dot(a, b) / (norm(a) * norm(b))\n\n# Create embeddings for three different questions\nq1 = embedder.encode([\"What time do you close?\"])[0]\nq2 = embedder.encode([\"When does the library shut?\"])[0]\nq3 = embedder.encode([\"How do I borrow a laptop?\"])[0]\n\nprint(\"üîç Embedding Similarity Demo:\\n\")\nprint(\"This shows how the 'GPS coordinates for meaning' actually work:\\n\")\nprint(f\"Question 1: 'What time do you close?'\")\nprint(f\"Question 2: 'When does the library shut?'\")\nprint(f\"Similarity: {similarity(q1, q2):.3f} ‚Üí HIGH (same topic!)\\n\")\n\nprint(f\"Question 1: 'What time do you close?'\")\nprint(f\"Question 3: 'How do I borrow a laptop?'\")\nprint(f\"Similarity: {similarity(q1, q3):.3f} ‚Üí LOW (different topics)\\n\")\n\nprint(\"üí° Why this matters:\")\nprint(\"   This is how your semantic card catalog finds relevant FAQs!\")\nprint(\"   Higher score = closer in vector space = better match = better answer\")\nprint(\"\\n   Like a reference interview: understanding synonyms and rephrasing\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìä Visualization: FAQs by Category\n\n**What this shows:** How many questions we have in each category.\n\n**Your source evaluation skills apply here:** \n- Which topics have good coverage vs. gaps?\n- Does this represent diverse user needs?\n- Are there underrepresented categories that need attention?\n\n**This is collection development for your RAG knowledge base.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Count FAQs per category\n",
    "category_counts = data['category'].value_counts().sort_values(ascending=True)\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = plt.cm.Set3(range(len(category_counts)))\n",
    "category_counts.plot(kind='barh', color=colors, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "plt.title('üìä Number of FAQs by Category', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Number of FAQs', fontsize=12)\n",
    "plt.ylabel('Category', fontsize=12)\n",
    "plt.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add count labels on bars\n",
    "for i, v in enumerate(category_counts):\n",
    "    plt.text(v + 0.1, i, str(v), va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Insights:\")\n",
    "most_common = category_counts.idxmax()\n",
    "least_common = category_counts.idxmin()\n",
    "print(f\"   ‚Ä¢ Most FAQs: {most_common.title()} ({category_counts[most_common]} questions)\")\n",
    "print(f\"   ‚Ä¢ Fewest FAQs: {least_common.title()} ({category_counts[least_common]} questions)\")\n",
    "print(f\"   ‚Ä¢ Total coverage: {len(category_counts)} different categories\")\n",
    "print(\"\\n   Consider adding more FAQs to underrepresented categories!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üîç Test Retrieval Quality\n\n**What this does:** Shows how well our semantic card catalog matches questions to FAQs.\n\n**From the presentation:** This is the \"reference interview + shelf browsing\" step (Step 3 - RETRIEVE).\n\n**Your expertise matters here:** Good retrieval = good RAG answers. If it retrieves the wrong FAQ, the LLM will give a wrong answer - just like directing a patron to the wrong section of the library.\n\n**Look for:** Does the matched FAQ make sense for the question asked? This is evaluating the quality of your retrieval system."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "test_qs = [\n    \"What time do you close on Sunday?\",\n    \"Can I access JSTOR from my apartment?\",\n    \"How long can I keep a book?\",\n    \"Where's the quiet zone?\",\n    \"Do you have group study rooms?\"\n]\n\nprint(\"üîç Retrieval Quality Test:\\n\")\nprint(\"Testing if our semantic card catalog finds the right FAQ for each question...\\n\")\n\nfor q in test_qs:\n    emb = embedder.encode([q])[0]\n    res = collection.query(query_embeddings=[emb.tolist()], n_results=1)\n    \n    if res['documents'][0]:\n        matched = res['metadatas'][0][0]['question']\n        print(f\"‚ùì Asked: \\\"{q}\\\"\")\n        print(f\"‚úÖ Matched: \\\"{matched}\\\"\\n\")\n\nprint(\"üí° Good retrieval = relevant matches (like finding the right shelf)\")\nprint(\"   If matches look wrong, you might need:\")\nprint(\"   ‚Ä¢ Better FAQ coverage (collection development)\")\nprint(\"   ‚Ä¢ Different embeddings (better 'coordinates')\")\nprint(\"   ‚Ä¢ More context in questions/answers (richer metadata)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üéÆ Interactive: Test Your Own Questions\n\n**Try it yourself!** Change the question below and see how RAG performs.\n\n**As mentioned in the presentation:** This is your chance to audit the code and see what works (and what doesn't).\n\n**Experiment ideas:**\n- Try questions similar to existing FAQs\n- Try questions NOT in the FAQs (what happens when there's no source?)\n- Try different phrasings of the same question (test semantic understanding)\n- Try vague vs specific questions (does it still find the right answer?)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# üëá CHANGE THIS QUESTION!\nyour_q = \"Can I eat pizza in the library?\"\n\nprint(\"=\" * 80)\nprint(f\"‚ùì YOUR QUESTION: {your_q}\")\nprint(\"=\" * 80)\n\nprint(\"\\nüî¥ WITHOUT RAG (Generic LLM - 15-30% hallucination rate):\\n\")\nprint(no_rag_answer(your_q))\nprint(\"\\n‚ö†Ô∏è  Notice: Vague, unhelpful, could be wrong, NO CITATION\\n\")\n\nprint(\"=\" * 80)\n\nprint(\"\\nüü¢ WITH RAG (Grounded in YOUR FAQs - 2-5% hallucination rate):\\n\")\nprint(rag_answer(your_q))\nprint(\"\\n‚úÖ Notice: Specific, accurate, cited source - GROUNDED AND VERIFIABLE\\n\")\n\nprint(\"=\" * 80)\n\nprint(\"\\nüí° Try changing 'your_q' above to test different questions!\")\nprint(\"   Remember: RAG can only answer from sources in the knowledge base\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üß™ Experiment: Top 3 Results\n\n**What this shows:** RAG doesn't just find 1 match - it can retrieve multiple relevant FAQs.\n\n**Why this matters (from the presentation):** \n- More context = better answers (like pulling multiple books from the shelf)\n- Handles questions that span multiple FAQs\n- Shows how confident the system is (big gap between #1 and #2 = very confident)\n\n**Your reference skills:** Just like you might pull 3-4 relevant books for a patron, RAG can use multiple sources.\n\n**Try changing the question** to see different results!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# üëá CHANGE THIS!\nexp_q = \"How do I use databases off campus?\"\n\nemb = embedder.encode([exp_q])[0]\nres = collection.query(query_embeddings=[emb.tolist()], n_results=3)\n\nprint(f\"üîç Query: \\\"{exp_q}\\\"\\n\")\nprint(\"Top 3 Most Relevant FAQs from Semantic Card Catalog:\\n\")\n\nfor i, (doc, meta) in enumerate(zip(res['documents'][0], res['metadatas'][0]), 1):\n    print(f\"[{i}] Question: {meta['question']}\")\n    print(f\"    Answer: {doc[:100]}...\\n\")\n\nprint(\"üí° In production RAG, you'd give all 3 to the LLM for richer context!\")\nprint(\"   Like handing a patron 3 relevant books instead of just 1\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìÅ Upload Your Own FAQs (Optional)\n\nWant to try with your library's actual data?\n\n**Required format:** CSV with `question`, `answer`, `category` columns\n\n**Tip:** See `CSV_GENERATION_PROMPT.md` in the GitHub repo for a ChatGPT prompt that generates perfectly formatted CSVs from your existing docs!\n\n**This is where your expertise matters most:** The quality of your FAQs (source evaluation, clear answers, good coverage) directly impacts RAG quality."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these lines to upload your own CSV\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# custom_data = pd.read_csv(list(uploaded.keys())[0])\n",
    "# print(f\"‚úÖ Loaded {len(custom_data)} custom FAQs\")\n",
    "# \n",
    "# Then re-run cells 7-9 to rebuild the vector database with your data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# ü§ñ REAL LLM INTEGRATION (OPTIONAL)\n\n**‚ö†Ô∏è FOR LIVE DEMO:** The presenter will skip this section (API keys are private!)\n\n**FOR YOU TO TRY LATER:** You can add your own API key and test the full RAG pipeline.\n\n---\n\n**So far:** We've built the RAG *retrieval* part (finding relevant FAQs from your semantic card catalog).\n\n**Now:** Let's add the *generation* part (actual AI-powered answers)!\n\n**From the presentation (Slide 8):** This is Step 4 - GENERATE answers using ONLY retrieved docs + cite sources.\n\n## Two Options:\n\n1. **OpenAI API** (GPT-4o-mini) - Fast, high quality, $0.15 per 1M tokens\n2. **Google Gemini API** (Gemini 1.5 Flash) - **FREE tier, no credit card needed!**\n\n**Pick based on your needs:**\n- **Free tier/testing?** ‚Üí Gemini (generous free quota)\n- **Production/quality?** ‚Üí Either! Both are excellent\n- **No credit card?** ‚Üí Gemini (truly free tier)\n\n**Remember from the presentation:** Both produce \"grounded and verifiable\" outputs when given good sources.\n\n**üîí Security:** All API key cells use `getpass` so your key stays hidden (shows dots, not actual characters).\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Option 1: OpenAI API (GPT-4o-mini) - OPTIONAL\n\n**Why OpenAI?**\n- ‚ö° Very fast (<2 seconds)\n- üéØ High quality answers\n- üí∞ Cheap ($0.15 per 1M input tokens ‚âà $0.15 per 2,000 questions)\n- üîß Easy to use\n\n**Setup:**\n1. Get free API key: https://platform.openai.com/api-keys\n2. Run the cell below - it will prompt for your key (input hidden)\n3. Or press Enter to skip this section\n\n**Note:** You'll need to add a payment method, but usage is extremely cheap for testing.\n\n**üîí For live demos:** Your key input is hidden (shows dots, not characters) so you can safely paste from another monitor."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install OpenAI SDK\n!pip install -q openai\n\nfrom openai import OpenAI\nfrom getpass import getpass\n\nprint(\"üîë OpenAI API Key Setup (OPTIONAL)\")\nprint(\"=\" * 60)\nprint(\"üìñ Get your API key: https://platform.openai.com/api-keys\")\nprint(\"üîí Your input will be hidden (shows dots, not characters)\")\nprint(\"‚è≠Ô∏è  Or press Enter to skip this section\")\nprint(\"=\" * 60)\n\nOPENAI_API_KEY = getpass(\"\\nAPI Key: \").strip()\n\nif OPENAI_API_KEY:\n    try:\n        openai_client = OpenAI(api_key=OPENAI_API_KEY)\n        # Test the key with a simple request\n        openai_client.models.list()\n        print(\"\\n‚úÖ OpenAI configured! You can now run the OpenAI RAG cells below.\")\n    except Exception as e:\n        print(f\"\\n‚ùå Error configuring OpenAI: {e}\")\n        print(\"   Please check your API key and try again.\")\n        openai_client = None\nelse:\n    openai_client = None\n    print(\"\\n‚è≠Ô∏è  Skipped - You can try this later with your own key\")\n    print(\"   The demo (Part 1) works perfectly without any API keys!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def rag_answer_openai(question, n_results=2, model=\"gpt-4o-mini\"):\n    \"\"\"RAG with OpenAI GPT-4o-mini - Full pipeline from presentation.\"\"\"\n    # Check if OpenAI is configured\n    if 'openai_client' not in globals() or openai_client is None:\n        return \"‚è≠Ô∏è OpenAI not configured. Run the setup cell above and add your API key (or press Enter to skip).\"\n\n    # Step 1: RETRIEVE - Find relevant FAQs from semantic card catalog\n    query_emb = embedder.encode([question])[0]\n    results = collection.query(query_embeddings=[query_emb.tolist()], n_results=n_results)\n\n    if not results['documents'][0]:\n        return \"No information found in knowledge base.\"\n\n    # Step 2: Build context from retrieved docs with sources\n    context_parts = []\n    for i, (doc, meta) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n        context_parts.append(f\"Source {i+1} (FAQ: '{meta['question']}'):\\n{doc}\")\n    context = \"\\n\\n\".join(context_parts)\n\n    # Step 3: GENERATE - Call OpenAI to create answer based ONLY on context\n    response = openai_client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful library assistant. Answer questions using ONLY the provided sources. Always cite which source you used. Be concise and accurate. If the sources don't contain the answer, say so.\"},\n            {\"role\": \"user\", \"content\": f\"Question: {question}\\n\\nSources:\\n{context}\\n\\nAnswer the question using ONLY the sources above. Cite which source(s) you used.\"}\n        ],\n        temperature=0.3,  # Lower = more conservative/factual\n        max_tokens=200\n    )\n\n    return response.choices[0].message.content\n\nprint(\"‚úÖ OpenAI RAG function ready!\")\nprint(\"\\nüí° This implements the four-step RAG pipeline from the presentation:\")\nprint(\"   1. INGEST: Already done (loaded FAQs)\")\nprint(\"   2. STORE: Already done (semantic card catalog)\")\nprint(\"   3. RETRIEVE: Search for relevant sources\")\nprint(\"   4. GENERATE: Create grounded, verifiable answer with citations\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test OpenAI RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# üëá CHANGE THIS!\ntest_q = \"Can I renew a book that's already late?\"\n\nprint(\"=\" * 80)\nprint(f\"‚ùì QUESTION: {test_q}\")\nprint(\"=\" * 80)\nprint(\"\\nü§ñ OPENAI GPT-4o-mini RAG ANSWER:\\n\")\n\nanswer = rag_answer_openai(test_q)\nprint(answer)\n\nif not answer.startswith(\"‚è≠Ô∏è\"):\n    print(\"\\n\" + \"=\" * 80)\n    print(\"\\n‚ö° Fast! Natural! Cited! GROUNDED AND VERIFIABLE!\")\n    print(\"   This is production-ready RAG - 2-5% hallucination rate vs 15-30%\")\nelse:\n    print(\"\\nüí° To enable OpenAI: Run the setup cell above with your API key\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Option 2: Google Gemini API (Gemini 1.5 Flash) - OPTIONAL\n\n**Why Gemini?**\n- üÜì Generous free tier (15 requests/minute, 1M requests/day!)\n- ‚ö° Very fast (<2 seconds)\n- üéØ High quality (comparable to GPT-4o-mini)\n- üí≥ **No credit card required for free tier!**\n\n**Setup:**\n1. Get free API key: https://aistudio.google.com/app/apikey\n2. Run the cell below - it will prompt for your key (input hidden)\n3. Or press Enter to skip this section\n\n**Perfect for:** Testing, prototyping, or low-volume production (<1M requests/day)\n\n**üîí For live demos:** Your key input is hidden (shows dots, not characters) so you can safely paste from another monitor."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install Google Generative AI SDK\n!pip install -q google-generativeai\n\nimport google.generativeai as genai\nfrom getpass import getpass\n\nprint(\"üîë Google Gemini API Key Setup (OPTIONAL)\")\nprint(\"=\" * 60)\nprint(\"üìñ Get your FREE API key: https://aistudio.google.com/app/apikey\")\nprint(\"üí≥ No credit card required!\")\nprint(\"üîí Your input will be hidden (shows dots, not characters)\")\nprint(\"‚è≠Ô∏è  Or press Enter to skip this section\")\nprint(\"=\" * 60)\n\nGEMINI_API_KEY = getpass(\"\\nAPI Key: \").strip()\n\nif GEMINI_API_KEY:\n    try:\n        genai.configure(api_key=GEMINI_API_KEY)\n        gemini_model = genai.GenerativeModel('gemini-1.5-flash')\n        # Test the key with a simple request\n        gemini_model.generate_content(\"test\")\n        print(\"\\n‚úÖ Gemini configured! You can now run the Gemini RAG cells below.\")\n    except Exception as e:\n        print(f\"\\n‚ùå Error configuring Gemini: {e}\")\n        print(\"   Please check your API key and try again.\")\n        gemini_model = None\nelse:\n    gemini_model = None\n    print(\"\\n‚è≠Ô∏è  Skipped - You can try this later with your own key\")\n    print(\"   The demo (Part 1) works perfectly without any API keys!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def rag_answer_gemini(question, n_results=2):\n    \"\"\"RAG with Google Gemini 1.5 Flash - Full pipeline from presentation.\"\"\"\n    # Check if Gemini is configured\n    if 'gemini_model' not in globals() or gemini_model is None:\n        return \"‚è≠Ô∏è Gemini not configured. Run the setup cell above and add your API key (or press Enter to skip).\"\n\n    # Step 1: RETRIEVE - Find relevant FAQs from semantic card catalog\n    query_emb = embedder.encode([question])[0]\n    results = collection.query(query_embeddings=[query_emb.tolist()], n_results=n_results)\n\n    if not results['documents'][0]:\n        return \"No information found in knowledge base.\"\n\n    # Step 2: Build context from retrieved docs with sources\n    context_parts = []\n    for i, (doc, meta) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n        context_parts.append(f\"Source {i+1} (FAQ: '{meta['question']}'):\\n{doc}\")\n    context = \"\\n\\n\".join(context_parts)\n\n    # Step 3: GENERATE - Build prompt for Gemini\n    prompt = f\"\"\"You are a helpful library assistant. Answer the question using ONLY the provided sources. Always cite which source you used. Be concise and accurate. If the sources don't contain the answer, say so.\n\nQuestion: {question}\n\nSources:\n{context}\n\nAnswer the question using ONLY the sources above. Cite which source(s) you used.\"\"\"\n\n    # Step 4: Call Gemini to generate grounded answer\n    response = gemini_model.generate_content(\n        prompt,\n        generation_config=genai.types.GenerationConfig(\n            temperature=0.3,  # Lower = more conservative/factual\n            max_output_tokens=200,\n        )\n    )\n\n    return response.text\n\nprint(\"‚úÖ Gemini RAG function ready!\")\nprint(\"\\nüí° This implements the four-step RAG pipeline from the presentation:\")\nprint(\"   1. INGEST: Already done (loaded FAQs)\")\nprint(\"   2. STORE: Already done (semantic card catalog)\")\nprint(\"   3. RETRIEVE: Search for relevant sources\")\nprint(\"   4. GENERATE: Create grounded, verifiable answer with citations\")\nprint(\"\\nüÜì Free tier: 15 requests/min, 1M requests/day!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Gemini RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# üëá CHANGE THIS!\ntest_q = \"How do I access databases from my apartment?\"\n\nprint(\"=\" * 80)\nprint(f\"‚ùì QUESTION: {test_q}\")\nprint(\"=\" * 80)\nprint(\"\\nü§ñ GOOGLE GEMINI RAG ANSWER:\\n\")\n\nanswer = rag_answer_gemini(test_q)\nprint(answer)\n\nif not answer.startswith(\"‚è≠Ô∏è\"):\n    print(\"\\n\" + \"=\" * 80)\n    print(\"\\n‚ö° Fast! Natural! Cited! GROUNDED AND VERIFIABLE!\")\n    print(\"   Free tier - perfect for testing and prototyping\")\nelse:\n    print(\"\\nüí° To enable Gemini: Run the setup cell above with your API key\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üÜö Compare Both LLMs\n",
    "\n",
    "Try the same question with both and see how they compare!\n",
    "\n",
    "**What to look for:**\n",
    "- Which one cites sources better?\n",
    "- Which one is more concise?\n",
    "- Which one sounds more natural?\n",
    "- Any hallucinations (info not in the sources)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# üëá CHANGE THIS!\ncompare_q = \"What are the late fees for overdue books?\"\n\nprint(\"=\" * 80)\nprint(f\"‚ùì QUESTION: {compare_q}\")\nprint(\"=\" * 80)\n\n# Try OpenAI (if configured)\nprint(\"\\nüü¢ OPENAI GPT-4o-mini:\\n\")\nopenai_answer = rag_answer_openai(compare_q)\nprint(openai_answer)\n\nprint(\"\\n\" + \"=\" * 80)\n\n# Try Gemini (if configured)\nprint(\"\\nüîµ GOOGLE GEMINI 1.5 Flash:\\n\")\ngemini_answer = rag_answer_gemini(compare_q)\nprint(gemini_answer)\n\nprint(\"\\n\" + \"=\" * 80)\n\n# Show observations if at least one is configured\nif not openai_answer.startswith(\"‚è≠Ô∏è\") or not gemini_answer.startswith(\"‚è≠Ô∏è\"):\n    print(\"\\nüí° Observations:\")\n    print(\"   ‚Ä¢ Both are fast (<2 seconds)\")\n    print(\"   ‚Ä¢ Both cite sources accurately\")\n    print(\"   ‚Ä¢ OpenAI: Requires payment method\")\n    print(\"   ‚Ä¢ Gemini: Truly free tier, no credit card needed\")\n    print(\"   ‚Ä¢ Quality: Both excellent for library FAQs!\")\nelse:\n    print(\"\\nüí° To enable comparison: Run setup cells for OpenAI and/or Gemini above\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## üéì What You Learned\n\nCongratulations! You just built a **production-ready RAG system**!\n\n### Key Concepts You Mastered:\n\n1. ‚úÖ **Embeddings** - GPS coordinates for meaning (Slide 11)\n2. ‚úÖ **Semantic Card Catalog** - Vector database for meaning-based search (Slide 10)\n3. ‚úÖ **Reference Interview + Shelf Browsing** - Retrieval (Step 3)\n4. ‚úÖ **Grounded and Verifiable Outputs** - Generation with citations (Step 4)\n5. ‚úÖ **Full RAG Pipeline** - All four steps working together\n\n### The Big Idea (from the presentation):\n\n> **RAG = Open-Book Exam for AI**\n>\n> Instead of hallucinating from memory, the AI looks up the answer in YOUR actual documents!\n\n> **\"RAG enhances generative AI by drawing on external sources... allowing outputs to be more grounded and verifiable.\"**  \n> ‚Äî ACRL AI Competencies, 2025\n\n### What Makes Good RAG - Your Expertise Matters:\n\n- üìö **Quality FAQs** - Your source evaluation skills\n- üéØ **Good Retrieval** - Your reference interview skills\n- ü§ñ **Smart LLM** - Natural, cited responses\n- ‚úÖ **Citations** - Your information literacy teaching\n\n### The Statistics (from Slide 3):\n\n| Metric | Standard LLM | With RAG | Improvement |\n|--------|-------------|----------|-------------|\n| **Hallucination Rate** | 15-30% | 2-5% | **6.5x more reliable** |\n| **Citations** | None | Always | **Verifiable** |\n| **Outdated Info** | Yes (2021-2023 cutoff) | No (your current docs) | **Always current** |\n\n### Trade-offs You Explored:\n\n| Feature | OpenAI | Gemini |\n|---------|--------|--------|\n| **Cost** | $0.15/1M tokens | Free tier! |\n| **Speed** | <2 sec | <2 sec |\n| **Quality** | Excellent | Excellent |\n| **Setup** | Needs payment | No credit card |\n| **Limits** | Pay as you go | 15/min free |\n\n**For libraries (from Slide 18):**\n- üß™ **Prototype** ‚Üí Gemini (free, no approvals needed)\n- üöÄ **Production** ‚Üí Either! Both work great\n- üîí **Privacy concerns** ‚Üí Local models (see resources)\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üöÄ Next Steps\n\n**From Slide 24 - Getting Started This Week:**\n\n### Immediate Next Steps (Easy - 2-3 hours):\n\n1. **Experiment with YOUR library's FAQs**\n   - Export from LibGuides/website\n   - Use CSV_GENERATION_PROMPT.md (in GitHub repo) with ChatGPT\n   - Upload and test!\n   - **Apply your source evaluation skills:** Which FAQs work well? Which don't?\n\n2. **Try public RAG tools**\n   - NotebookLM (Google) - upload your FAQ PDF\n   - Perplexity AI - see web + RAG\n   - ChatGPT - upload docs, ask questions\n   - **Evaluate critically:** Are citations accurate? Verifiable? Complete?\n\n3. **Experiment with this notebook**\n   - Try different questions\n   - Adjust n_results (more context)\n   - Change temperature (0.1 = conservative, 0.7 = creative)\n\n### Intermediate (Medium):\n\n4. **Add more document types**\n   - Policy PDFs\n   - Hours/locations\n   - Staff directory\n   - **Collection development for RAG:** What should be included?\n\n5. **Build a simple UI**\n   - Streamlit (easiest)\n   - Gradio (good for demos)\n   - Flask (more control)\n\n6. **Improve retrieval**\n   - Hybrid search (keywords + semantic)\n   - Reranking top results\n   - Better chunking strategies\n\n### Advanced (Hard):\n\n7. **Add conversation memory**\n   - Multi-turn dialogue\n   - Context from previous questions\n   - User session tracking\n\n8. **Deploy to production**\n   - HuggingFace Spaces (free!)\n   - Embed in LibGuides\n   - Slack/Teams bot\n\n9. **Advanced features**\n   - User feedback loop\n   - Analytics dashboard\n   - A/B testing\n   - Multilingual support\n\n### Remember from the Presentation (Slide 25):\n\n**Start Small, Stay Critical:**\n- ‚úÖ Pilots, not production (sandbox first)\n- ‚úÖ Ethics review before deployment\n- ‚úÖ Failure is okay - learning is goal\n- ‚úÖ Apply ACRL Section 3.2 framework: evaluate data quality, bias, output sufficiency\n\n**You Already Have RAG Expertise:**\n- Source evaluation = document quality\n- Cataloging = structuring knowledge  \n- Reference interviews = understanding queries\n- This is your professional wheelhouse!\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìö Resources\n\n### This Presentation:\n- **GitHub Repo:** [github.com/radio-shaq/Lyrasis-slides-11-2025](https://github.com/radio-shaq/Lyrasis-slides-11-2025)\n- **CSV Generation Guide:** See `CSV_GENERATION_PROMPT.md` in repo\n- **Slides + Speaker Notes:** PowerPoint and markdown versions in repo\n- **ACRL Framework Resources:** Links and guidance\n\n### Key Frameworks (from Presentation):\n\n**ACRL AI Competencies for Academic Library Workers (October 2025):**\n- Section 3.2: \"Evaluate benefits and risks in deployment of AI technologies\"\n- **Core principle:** \"RAG enhances generative AI... allowing outputs to be more grounded and verifiable\"\n- **Your role:** Apply information literacy to algorithmic systems\n- Link in GitHub repo\n\n### Research Citations (from Slide 3):\n\n- **Lewis et al. (2020):** Original RAG paper - \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" (Meta AI/UCL/NYU)\n- **TruthfulQA (Lin et al., 2022):** Baseline LLM hallucination rates (15-30%)\n- **Watanabe et al. (2025):** RAG hallucination rates with reliable sources (2-5%)\n\n### API Documentation:\n- **OpenAI:** [platform.openai.com/docs](https://platform.openai.com/docs)\n- **Google Gemini:** [ai.google.dev/docs](https://ai.google.dev/docs)\n- **ChromaDB:** [docs.trychroma.com](https://docs.trychroma.com)\n- **Sentence Transformers:** [sbert.net](https://www.sbert.net)\n\n### Learning More:\n- **Simon Willison's Blog:** Practical RAG experiments ([simonwillison.net](https://simonwillison.net))\n- **LangChain Docs:** Framework for LLM apps ([python.langchain.com](https://python.langchain.com))\n- **Pinecone Learning Center:** Vector DB tutorials ([pinecone.io/learn](https://www.pinecone.io/learn/))\n\n### Tools to Explore:\n- **LangChain:** RAG framework (makes this easier!)\n- **LlamaIndex:** Document-focused RAG\n- **Streamlit:** Easy web apps for demos\n- **Gradio:** Interactive ML demos\n\n### Library Examples (from Slide 9):\n- **Columbia University:** RAG-enhanced CLIO search (2024)\n- **Virginia Beach Public Library:** PAGE chatbot (110,000+ monthly engagements)\n- **NotebookLM:** Google's RAG tool for research\n\n### Questions?\n- **Email:** davidmeincke@protonmail.com\n- **GitHub Issues:** Open an issue in the repo!\n\n---\n\n## üéâ Thank You!\n\nYou now have:\n- ‚úÖ A working RAG demo\n- ‚úÖ Understanding of embeddings & semantic card catalogs\n- ‚úÖ Integration with real LLMs (OpenAI & Gemini)\n- ‚úÖ A foundation to build production systems\n- ‚úÖ The knowledge that YOUR librarian skills apply to evaluating these systems\n\n### Remember from the Presentation:\n\n**\"Your expertise maps to RAG\"** (Slide 5):\n- Source evaluation ‚Üí Document quality\n- Cataloging ‚Üí Structuring knowledge\n- Reference interviews ‚Üí Query understanding\n- Information literacy ‚Üí Critical AI evaluation\n\n**\"RAG needs librarian expertise\"** (Slide 26):\n- Evaluating sources\n- Understanding user needs\n- Teaching critical thinking\n- Bringing judgment to information systems\n\n**Go forth and build amazing library AI tools!** üöÄüìö\n\nApply your professional judgment. Stay critical. Start small.\n\n---\n\n*LYRASIS Presentation, November 2025*  \n*Built with ‚ù§Ô∏è for libraries*\n\n*\"I love how AI tools can help us build small shareable code projects much faster. But the librarian expertise - evaluating sources, understanding user needs, teaching critical thinking - that's timeless. RAG needs that. Students need that. You have that.\"*"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}